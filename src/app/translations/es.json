{
  "hackupc": "MATESDADDIES",
  "modelArchitectureTitle": "Arquitectura del Modelo",
  "title": "Recomendador de Moda",
  "subtitle": "Ver artículos similares de la colección de Inditex",
  "formsDefault": "Introduce un número",
  "recommenderTitle": "Recomendador",
  "recommenderSubtitle": "Encuentra artículos de la colección de Inditex",
  "architectureT2": "Preprocesamiento",
  "architectureP1": "En esta primera fase, nuestro objetivo es generar un conjunto de datos de embeddings de las imágenes.",
  "architectureT3": "Procesamiento de imágenes",
  "architectureP2": "Nos proporcionan una lista de URLs de imágenes. Las imágenes están en grupos de 3, cada grupo representa un producto. Como necesitaremos ejecutar inferencia y entrenamiento varias veces sobre estas imágenes, nuestro primer objetivo es descargar cada imagen para tener un conjunto de datos local. Después de lidiar varias veces con la protección DDoS, logramos descargar el conjunto de datos completo. El único inconveniente fue alrededor de 1000 imágenes corruptas de 140k, lo cual nos deja satisfechos con el conjunto de datos.",
  "architectureT4": "Segmentación de ropa",
  "architectureP3": "Al descubrir que cada imagen podría tener diferentes fondos, configuraciones y perspectivas, decidimos usar un modelo de segmentación de ropa para permitir que el modelo se concentre en la parte importante de cada imagen. Para cada imagen, la procesamos a través del modelo para obtener la máscara que define la región de interés, y cambiamos el fondo de la imagen original por una pantalla verde (por muy feo que sea este color, el negro puede llevar a algunos problemas al analizar ropa oscura). Este proceso se realiza con un modelo U2NET preentrenado para el análisis de ropa en retratos humanos.",
  "architectureT5": "Incrustación de imágenes",
  "architectureP4": "Con el conjunto de datos de imágenes enmascaradas, luego queremos obtener sus embeddings (representaciones vectoriales) para medir la similitud entre diferentes prendas usando la medida de similitud del coseno. Para ese propósito, usamos un modelo tipo CLIP preentrenado, ajustado específicamente para moda en un conjunto de datos de Zalando de 71k muestras etiquetadas.",
  "architectureT6": "Carga en MongoDB",
  "architectureP5": "Finalmente, normalizamos los embeddings y los promediamos sobre las imágenes que pertenecen al mismo grupo. Los vectores resultantes se asocian con la URL original de las imágenes del grupo en un dataframe, que luego se carga en MongoDB. Esta herramienta nos permite luego leer la información desde el front-end y buscar de manera eficiente las coincidencias de similitud top-k. Proporcionamos un esquema del pipeline para resumir lo que se ha dicho.",
  "architectureT7": "Inferencia",
  "architectureP6": "Con un conjunto de datos de embeddings, asociados a las imágenes, y la capacidad de buscar de manera eficiente en el espacio para encontrar ropa similar, estamos completamente preparados para la inferencia. En nuestro front-end, tienes la capacidad de elegir las características de la prenda que quieres encontrar. Esta información, gracias a la elección de usar un modelo CLIP, puede ser incrustada en el mismo espacio que las imágenes. El resultado más similar se te devuelve, incluyendo todas sus 3 imágenes. Además, usamos el resultado principal para encontrar los 5 que son más similares a él. Con esto, nuestro objetivo es ofrecer alternativas que se parezcan al original. Aquí tienes un esquema de esto:"
}