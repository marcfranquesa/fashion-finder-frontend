{
  "hackupc": "MATESDADDIES",
  "modelArchitectureTitle": "Arquitectura del Modelo",
  "title": "Buscador de Moda",
  "subtitle": "Ver artículos similares de la colección de Inditex",
  "image-selected": "Producto seleccionado:",
  "image-recommendations": "Productos recomendados:",
  "formsDefault": "Ingrese un id de producto",
  "recommenderTitle": "Recomendador",
  "recommenderSubtitle": "Encuentra artículos de la colección de Inditex",
  "architectureT2": "Preprocesamiento",
  "architectureP1": "En esta primera fase, nuestro objetivo es generar un conjunto de datos de incrustaciones de las imágenes.",
  "architectureT3": "Procesamiento de imágenes",
  "architectureP2": "Nos proporcionan una lista de URLs de imágenes. Las imágenes están en grupos de 3, cada grupo representa un producto. Como necesitaremos ejecutar inferencia y entrenamiento varias veces sobre estas imágenes, nuestro primer objetivo es descargar cada imagen para tener un conjunto de datos local. Después de lidiar varias veces con la protección DDoS, logramos descargar el conjunto de datos completo. El único inconveniente fueron alrededor de 1000 imágenes corruptas de 140k, lo cual nos deja satisfechos con el conjunto de datos.",
  "architectureT4": "Segmentación de ropa",
  "architectureP3": "Al descubrir que cada imagen podría tener diferentes fondos, configuraciones y perspectivas, decidimos usar un modelo de segmentación de ropa para permitir que el modelo se concentre en la parte importante de cada imagen. Para cada imagen, la ejecutamos a través del modelo para recuperar la máscara que define la región de interés, y cambiamos el fondo de la imagen original por una pantalla verde (por feo que pueda ser este color, el negro puede llevar a algunos problemas al analizar ropa oscura). Este proceso se realiza con un modelo U2NET preentrenado para el análisis de ropa en retrato humano.",
  "architectureT5": "Incrustación de imágenes",
  "architectureP4": "Con el conjunto de datos de imágenes enmascaradas, luego queremos obtener sus incrustaciones (representaciones vectoriales) para medir la similitud entre diferentes prendas usando la medida de similitud del coseno. Para ese propósito, usamos un modelo tipo CLIP preentrenado, afinado para la moda en un conjunto de datos de Zalando de 71k muestras etiquetadas.",
  "architectureT6": "Carga en MongoDB",
  "architectureP5": "Finalmente, normalizamos las incrustaciones y las promediamos sobre las imágenes que pertenecen al mismo grupo. Los vectores resultantes se asocian con la URL original de las imágenes del grupo en un dataframe, que luego se carga en MongoDB. Esta herramienta nos permite luego leer la información desde el front-end y buscar de manera eficiente las coincidencias de similitud top-k. Proporcionamos un esquema del pipeline para resumir lo que se ha dicho.",
  "architectureT7": "Inferencia",
  "architectureP6": "Con un conjunto de datos de embeddings asociados a las imágenes y la capacidad de buscar eficientemente en el espacio por ropa similar, estamos listos para la inferencia. En nuestro front-end, tienes la capacidad de elegir las características de la prenda que deseas encontrar. Esta información, gracias a la elección de usar un modelo CLIP, puede ser integrada en el mismo espacio que las imágenes. El resultado más similar se te devuelve, incluyendo todas sus 3 imágenes. Además, utilizamos el mejor resultado para encontrar los 5 más similares a este. Con esto, nuestro objetivo es ofrecer alternativas que se parezcan al original. Aquí tienes un esbozo de esto:"
}